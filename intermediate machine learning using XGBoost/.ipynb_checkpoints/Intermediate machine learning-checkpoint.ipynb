{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>INTERMEDIATE MACHINE LEARNING</h1>\n",
    "<p>Contents</p>\n",
    "<ul>\n",
    "<li>Review</li>\n",
    "<li>Missing values</li>\n",
    "<li>Categorical Variables</li>\n",
    "<li>Pipe lines</li>\n",
    "<li>Cross validation</li>\n",
    "<li>XG boost</li>\n",
    "<li>Data leakage</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Review<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Suburb</th>\n",
       "      <th>Address</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Method</th>\n",
       "      <th>SellerG</th>\n",
       "      <th>Date</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>...</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>CouncilArea</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>85 Turner St</td>\n",
       "      <td>2</td>\n",
       "      <td>h</td>\n",
       "      <td>1480000.0</td>\n",
       "      <td>S</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>3/12/2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.7996</td>\n",
       "      <td>144.9984</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>25 Bloomburg St</td>\n",
       "      <td>2</td>\n",
       "      <td>h</td>\n",
       "      <td>1035000.0</td>\n",
       "      <td>S</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>4/02/2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8079</td>\n",
       "      <td>144.9934</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>5 Charles St</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>1465000.0</td>\n",
       "      <td>SP</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>4/03/2017</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8093</td>\n",
       "      <td>144.9944</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>40 Federation La</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>850000.0</td>\n",
       "      <td>PI</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>4/03/2017</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.7969</td>\n",
       "      <td>144.9969</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>55a Park St</td>\n",
       "      <td>4</td>\n",
       "      <td>h</td>\n",
       "      <td>1600000.0</td>\n",
       "      <td>VB</td>\n",
       "      <td>Nelson</td>\n",
       "      <td>4/06/2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8072</td>\n",
       "      <td>144.9941</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Suburb           Address  Rooms Type      Price Method SellerG  \\\n",
       "0  Abbotsford      85 Turner St      2    h  1480000.0      S  Biggin   \n",
       "1  Abbotsford   25 Bloomburg St      2    h  1035000.0      S  Biggin   \n",
       "2  Abbotsford      5 Charles St      3    h  1465000.0     SP  Biggin   \n",
       "3  Abbotsford  40 Federation La      3    h   850000.0     PI  Biggin   \n",
       "4  Abbotsford       55a Park St      4    h  1600000.0     VB  Nelson   \n",
       "\n",
       "        Date  Distance  Postcode  ...  Bathroom  Car  Landsize  BuildingArea  \\\n",
       "0  3/12/2016       2.5    3067.0  ...       1.0  1.0     202.0           NaN   \n",
       "1  4/02/2016       2.5    3067.0  ...       1.0  0.0     156.0          79.0   \n",
       "2  4/03/2017       2.5    3067.0  ...       2.0  0.0     134.0         150.0   \n",
       "3  4/03/2017       2.5    3067.0  ...       2.0  1.0      94.0           NaN   \n",
       "4  4/06/2016       2.5    3067.0  ...       1.0  2.0     120.0         142.0   \n",
       "\n",
       "   YearBuilt  CouncilArea Lattitude  Longtitude             Regionname  \\\n",
       "0        NaN        Yarra  -37.7996    144.9984  Northern Metropolitan   \n",
       "1     1900.0        Yarra  -37.8079    144.9934  Northern Metropolitan   \n",
       "2     1900.0        Yarra  -37.8093    144.9944  Northern Metropolitan   \n",
       "3        NaN        Yarra  -37.7969    144.9969  Northern Metropolitan   \n",
       "4     2014.0        Yarra  -37.8072    144.9941  Northern Metropolitan   \n",
       "\n",
       "  Propertycount  \n",
       "0        4019.0  \n",
       "1        4019.0  \n",
       "2        4019.0  \n",
       "3        4019.0  \n",
       "4        4019.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "melbourne_data_path=\"S:\\playing with data with python\\kaggle course for _intro to machine learning_ and other resourses\\input_data\\melbourne_housing\\melb_data.csv\"\n",
    "melbourne_data_path2=r\"S:\\playing with data with python\\kaggle course for _intro to machine learning_ and other resourses\\input_data\\home-data-for-ml-course\\train_data.csv\"\n",
    "dataset=pd.read_csv(melbourne_data_path)\n",
    "dataset2=pd.read_csv(melbourne_data_path2)\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random forest regressor model</h3>\n",
    "<p>first of all  we will make the ML model. Then we will test the techniques to reduce the effect caused due to missing values</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y=dataset.Price\n",
    "features = ['Landsize', 'Rooms', 'Distance', 'Bathroom', 'Propertycount','Car','BuildingArea','YearBuilt']\n",
    "X=dataset[features]\n",
    "train_x,val_x,train_y,val_y=train_test_split(X,y,random_state=0)\n",
    "model=RandomForestRegressor(random_state=1)\n",
    "# model.fit(train_x,train_y)\n",
    "# model_prediction=model.predict(val_x)\n",
    "# accuracy=mean_absolute_error(val_y,model_prediction)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>How to deal with MISSING VALUES</h2>\n",
    "<p>Some of the methods used for missing values are</p>\n",
    "<ol>\n",
    "    <li><b>A Simple Option: Drop Columns with Missing Values</b><p>Unless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!) information with this approach. </p></li>\n",
    "    <li><b>A Better Option: Imputation</b><p>Imputation fills in the missing values with some number. For instance, we can fill in the mean value along each column. The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely.</p></li>\n",
    "    <li><b>An Extension To Imputation</b><p>Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing.In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.\n",
    "\n",
    "In some cases, this will meaningfully improve results. In other cases, it doesn't help at all.</p></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Comparing all the methods for the melbourne dataset</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Drop comuns with missing values</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in train_x.columns\n",
    "                     if train_x[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = train_x.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = val_x.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Landsize  Rooms  Distance  Bathroom  Propertycount\n",
      "664       368.0      3       9.2       2.0         7809.0\n",
      "3270      586.0      2      10.5       1.0         2947.0\n",
      "3873      348.0      2      11.2       1.0         8801.0\n",
      "13170     521.0      3      19.6       1.0        10926.0\n",
      "1730      687.0      4      11.4       2.0         7822.0\n",
      "...         ...    ...       ...       ...            ...\n",
      "13123     212.0      3       5.2       1.0        11918.0\n",
      "3264      748.0      3      10.5       1.0         2947.0\n",
      "9845      441.0      4       6.7       2.0        11204.0\n",
      "10799     606.0      3      12.0       1.0        21650.0\n",
      "2732      319.0      4       6.4       2.0         7570.0\n",
      "\n",
      "[10185 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(reduced_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>IMPUTATION</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model acuracy =  224442.8945524291 by dropping the missing values columns\n"
     ]
    }
   ],
   "source": [
    "model.fit(reduced_X_train,train_y)\n",
    "model_prediction=model.predict(reduced_X_valid)\n",
    "accuracy=mean_absolute_error(val_y,model_prediction)\n",
    "print(\"model acuracy = \",accuracy,\"by dropping the missing values columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0    1     2    3        4    5           6            7\n",
      "0      368.0  3.0   9.2  2.0   7809.0  2.0  177.000000  2009.000000\n",
      "1      586.0  2.0  10.5  1.0   2947.0  2.0   80.000000  1955.000000\n",
      "2      348.0  2.0  11.2  1.0   8801.0  1.0  154.655601  1964.938304\n",
      "3      521.0  3.0  19.6  1.0  10926.0  1.0  154.655601  1964.938304\n",
      "4      687.0  4.0  11.4  2.0   7822.0  2.0  237.000000  1983.000000\n",
      "...      ...  ...   ...  ...      ...  ...         ...          ...\n",
      "10180  212.0  3.0   5.2  1.0  11918.0  2.0  154.655601  1964.938304\n",
      "10181  748.0  3.0  10.5  1.0   2947.0  1.0  101.000000  1950.000000\n",
      "10182  441.0  4.0   6.7  2.0  11204.0  2.0  255.000000  2002.000000\n",
      "10183  606.0  3.0  12.0  1.0  21650.0  1.0  154.655601  1964.938304\n",
      "10184  319.0  4.0   6.4  2.0   7570.0  1.0  130.000000  1915.000000\n",
      "\n",
      "[10185 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer=SimpleImputer()\n",
    "imputed_x_train=pd.DataFrame(imputer.fit_transform(train_x))     #Its necessary to fit transform this\n",
    "imputed_x_valid=pd.DataFrame(imputer.transform(val_x))\n",
    "print(imputed_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>As imputation removes the column names we have to re add it</b></p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Landsize  Rooms  Distance  Bathroom  Propertycount  Car  BuildingArea  \\\n",
      "0         368.0    3.0       9.2       2.0         7809.0  2.0    177.000000   \n",
      "1         586.0    2.0      10.5       1.0         2947.0  2.0     80.000000   \n",
      "2         348.0    2.0      11.2       1.0         8801.0  1.0    154.655601   \n",
      "3         521.0    3.0      19.6       1.0        10926.0  1.0    154.655601   \n",
      "4         687.0    4.0      11.4       2.0         7822.0  2.0    237.000000   \n",
      "...         ...    ...       ...       ...            ...  ...           ...   \n",
      "10180     212.0    3.0       5.2       1.0        11918.0  2.0    154.655601   \n",
      "10181     748.0    3.0      10.5       1.0         2947.0  1.0    101.000000   \n",
      "10182     441.0    4.0       6.7       2.0        11204.0  2.0    255.000000   \n",
      "10183     606.0    3.0      12.0       1.0        21650.0  1.0    154.655601   \n",
      "10184     319.0    4.0       6.4       2.0         7570.0  1.0    130.000000   \n",
      "\n",
      "         YearBuilt  \n",
      "0      2009.000000  \n",
      "1      1955.000000  \n",
      "2      1964.938304  \n",
      "3      1964.938304  \n",
      "4      1983.000000  \n",
      "...            ...  \n",
      "10180  1964.938304  \n",
      "10181  1950.000000  \n",
      "10182  2002.000000  \n",
      "10183  1964.938304  \n",
      "10184  1915.000000  \n",
      "\n",
      "[10185 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "imputed_x_train.columns=train_x.columns\n",
    "imputed_x_valid.columns=val_x.columns\n",
    "print(imputed_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Now we will fit the model based on the imputed data</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy=  225043.2153291382 by imputaion\n"
     ]
    }
   ],
   "source": [
    "model.fit(imputed_x_train,train_y)\n",
    "model_prediction=model.predict(imputed_x_valid)\n",
    "model_accuracy=mean_absolute_error(val_y,model_prediction)\n",
    "print(\"model accuracy= \",model_accuracy,\"by imputaion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Extension to Imputation</u></h3>\n",
    "<p>but this doesnt work well</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy =  224895.92135761146  for an extension to imputation\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = train_x.copy()\n",
    "X_valid_plus = val_x.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "model.fit(imputed_X_train_plus,train_y)\n",
    "model_predictions=model.predict(imputed_X_valid_plus)\n",
    "acurracy=mean_absolute_error(val_y,model_predictions)\n",
    "print(\"The accuracy = \",acurracy,\" for an extension to imputation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10185, 8)\n",
      "Car               47\n",
      "BuildingArea    4843\n",
      "YearBuilt       4042\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(train_x.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (train_x.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Rows = 10185<br>\n",
    "Columns = 8</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest percentage of missing values in a column =  47.550319096710844\n"
     ]
    }
   ],
   "source": [
    "print(\"Highest percentage of missing values in a column = \",(4843/10185)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Since the highest percentage of missing values in a column is greater than 20% it would be a good idea to drop the columnn as it would not contribute much to the process. Still then dropping the columns and imputation have been discussed above</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Given that thre are so few missing values in the dataset, we'd expect imputation to perform better than dropping columns entirely. However, we see that dropping columns performs slightly better! While this can probably partially be attributed to noise in the dataset, another potential explanation is that the imputation method is not a great match to this dataset. That is, maybe instead of filling in the mean value, it makes more sense to set every missing value to a value of 0, to fill in the most frequently encountered value, or to use some other method. For instance, consider the GarageYrBlt column (which indicates the year that the garage was built). It's likely that in some cases, a missing value could indicate a house that does not have a garage. Does it make more sense to fill in the median value along each column in this case? Or could we get better results by filling in the minimum value along each column? It's not quite clear what's best in this case, but perhaps we can rule out some options immediately - for instance, setting missing values in this column to 0 is likely to yield horrible results!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Imputatiuon using Median values</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPUTATION WITH MEDIAN VALUES\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Imputation\n",
    "final_imputer = SimpleImputer(strategy='median')\n",
    "final_X_train = pd.DataFrame(final_imputer.fit_transform(train_x))\n",
    "final_X_valid = pd.DataFrame(final_imputer.transform(val_x))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "final_X_train.columns = train_x.columns\n",
    "final_X_valid.columns = val_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of median imputer = \n",
      "212566.91682861804\n"
     ]
    }
   ],
   "source": [
    "#define and fit the model\n",
    "model = RandomForestRegressor(n_estimators = 100, random_state=0)\n",
    "model.fit(final_X_train,train_y)\n",
    "\n",
    "#get prediction and validation using mae\n",
    "prediction = model.predict(final_X_valid)\n",
    "print(\"MAE of median imputer = \")\n",
    "print(mean_absolute_error(val_y,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>There is a significant reduction in MAE with this median imputer approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Final preprocessing and the results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error =  212566.91682861804\n"
     ]
    }
   ],
   "source": [
    "final_X_test = pd.DataFrame(final_imputer.transform(val_x))\n",
    "prediction = model.predict(final_X_test)\n",
    "mae_error=mean_absolute_error(val_y,prediction)\n",
    "print(\"Mean absolute error = \",mae_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions to file\n",
    "output = pd.DataFrame({'Id': val_x.index,\n",
    "                       'SalePrice': prediction})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>CATEGORICAL VARIABLES</h2>\n",
    "<p>Lets talk what these are.</p><br>\n",
    "<p>A<b> Categorical variable</b> only takes a limited number of values.<br>For example if a survey ask you what type of cars you drove among : honda, ford ,toyota . Your answer will be limited to only a fixed set of values</p>\n",
    "<br>\n",
    "<h4>Three approaches to deal with categorical values</h4>\n",
    "<ol>\n",
    "    <li>Drop categorical variables if they are not usefull</li>\n",
    "    <br>\n",
    "    <li>Label encode the variables if you want to substitute the values with a number that provide a indisputable ranking to the categories.</li>\n",
    "    <br>\n",
    "    <li>One hot encodeing : creates new columns indicating the presence (or absence) of each possible value in the original data.<br>\n",
    "        <img src=\"https://i.imgur.com/TW5m0aJ.png\"><br>\n",
    "        <p>In contrast to label encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither more nor less than \"Yellow\"). We refer to categorical variables without an intrinsic ranking as nominal variables.\n",
    "\n",
    "One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).</p>\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "              Suburb           Address Type Method   SellerG        Date  \\\n",
      "0         Abbotsford      85 Turner St    h      S    Biggin   3/12/2016   \n",
      "1         Abbotsford   25 Bloomburg St    h      S    Biggin   4/02/2016   \n",
      "2         Abbotsford      5 Charles St    h     SP    Biggin   4/03/2017   \n",
      "3         Abbotsford  40 Federation La    h     PI    Biggin   4/03/2017   \n",
      "4         Abbotsford       55a Park St    h     VB    Nelson   4/06/2016   \n",
      "...              ...               ...  ...    ...       ...         ...   \n",
      "13575  Wheelers Hill      12 Strada Cr    h      S     Barry  26/08/2017   \n",
      "13576   Williamstown     77 Merrett Dr    h     SP  Williams  26/08/2017   \n",
      "13577   Williamstown       83 Power St    h      S     Raine  26/08/2017   \n",
      "13578   Williamstown      96 Verdon St    h     PI   Sweeney  26/08/2017   \n",
      "13579     Yarraville        6 Agnes St    h     SP   Village  26/08/2017   \n",
      "\n",
      "      CouncilArea                  Regionname  \n",
      "0           Yarra       Northern Metropolitan  \n",
      "1           Yarra       Northern Metropolitan  \n",
      "2           Yarra       Northern Metropolitan  \n",
      "3           Yarra       Northern Metropolitan  \n",
      "4           Yarra       Northern Metropolitan  \n",
      "...           ...                         ...  \n",
      "13575         NaN  South-Eastern Metropolitan  \n",
      "13576         NaN        Western Metropolitan  \n",
      "13577         NaN        Western Metropolitan  \n",
      "13578         NaN        Western Metropolitan  \n",
      "13579         NaN        Western Metropolitan  \n",
      "\n",
      "[13580 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "\n",
    "# Read the data\n",
    "X = pd.read_csv('home-data-for-ml-course/train.csv', index_col='Id') \n",
    "X_test = pd.read_csv('home-data-for-ml-course/test.csv', index_col='Id')\n",
    "\n",
    "\n",
    "s = (dataset.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll drop columns with missing values\n",
    "cols_with_missing = [col for col in X.columns if X[col].isnull().any()] \n",
    "X.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_test.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(dataset[object_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break off validation set from training data\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y,\n",
    "                                                      train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>METHOD 1: <u>DROP CATEGORICAL VARIABLES</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "17837.82570776256\n",
      "\n",
      " 38544  X  292\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dropping the categorical values\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "model.fit(drop_X_train,Y_train)                                      \n",
    "pred = model.predict(drop_X_valid)\n",
    "mae = mean_absolute_error(pred,Y_valid)\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(mae)\n",
    "\n",
    "print(\"\\n\",drop_X_train.size,\" X \",Y_valid.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MAE by dropping categorical values = 19018.395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>20</td>\n",
       "      <td>11694</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1774</td>\n",
       "      <td>1822</td>\n",
       "      <td>...</td>\n",
       "      <td>774</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>20</td>\n",
       "      <td>6600</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>894</td>\n",
       "      <td>894</td>\n",
       "      <td>...</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>13360</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1921</td>\n",
       "      <td>2006</td>\n",
       "      <td>713</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>876</td>\n",
       "      <td>...</td>\n",
       "      <td>432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>20</td>\n",
       "      <td>13265</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>1568</td>\n",
       "      <td>...</td>\n",
       "      <td>857</td>\n",
       "      <td>150</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>20</td>\n",
       "      <td>13704</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1541</td>\n",
       "      <td>1541</td>\n",
       "      <td>...</td>\n",
       "      <td>843</td>\n",
       "      <td>468</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MSSubClass  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
       "Id                                                                            \n",
       "619          20    11694            9            5       2007          2007   \n",
       "871          20     6600            5            5       1962          1962   \n",
       "93           30    13360            5            7       1921          2006   \n",
       "818          20    13265            8            5       2002          2002   \n",
       "303          20    13704            7            5       2001          2002   \n",
       "\n",
       "     BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  ...  GarageArea  \\\n",
       "Id                                                   ...               \n",
       "619          48           0       1774         1822  ...         774   \n",
       "871           0           0        894          894  ...         308   \n",
       "93          713           0        163          876  ...         432   \n",
       "818        1218           0        350         1568  ...         857   \n",
       "303           0           0       1541         1541  ...         843   \n",
       "\n",
       "     WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\n",
       "Id                                                                              \n",
       "619           0          108              0          0          260         0   \n",
       "871           0            0              0          0            0         0   \n",
       "93            0            0             44          0            0         0   \n",
       "818         150           59              0          0            0         0   \n",
       "303         468           81              0          0            0         0   \n",
       "\n",
       "     MiscVal  MoSold  YrSold  \n",
       "Id                            \n",
       "619        0       7    2007  \n",
       "871        0       8    2009  \n",
       "93         0       8    2009  \n",
       "818        0       7    2008  \n",
       "303        0       1    2006  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_X_train.dtypes\n",
    "drop_X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>METHOD 2: <u>LABEL ENCODING</u></h3>\n",
    "<p>Scikit-learn has a LabelEncoder class that can be used to get label encodings. We loop over the categorical variables and apply the label encoder separately to each column.</p>\n",
    "\n",
    "### Fitting a label encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Condition2' column in training data: ['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']\n",
      "\n",
      "Unique values in 'Condition2' column in validation data: ['Norm' 'RRAn' 'RRNn' 'Artery' 'Feedr' 'PosN']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\n",
    "print(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problematic columns can be dropped from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be label encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'BldgType', 'HouseStyle', 'ExterQual', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Exterior1st', 'ExterCond', 'RoofStyle', 'RoofMatl', 'SaleType', 'Utilities', 'HeatingQC', 'Neighborhood', 'LandSlope', 'Heating', 'Foundation', 'Condition2', 'Functional', 'Exterior2nd', 'Condition1']\n"
     ]
    }
   ],
   "source": [
    "# All categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_cols = [col for col in object_cols if \n",
    "                   set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_cols = list(set(object_cols)-set(good_cols))\n",
    "        \n",
    "print('Categorical columns that will be label encoded:', good_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Label Encoding):\n",
      "17575.291883561644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Make a copy to avoid loss of original data\n",
    "# label_X_train = X_train.copy()\n",
    "# label_X_test  = X_valid.copy()\n",
    "\n",
    "label_X_train = X_train.drop(bad_cols , axis = 1)\n",
    "label_X_valid = X_valid.drop(bad_cols , axis = 1)\n",
    "\n",
    "#Apply label encoder to each column with categorical data\n",
    "labelEncoder = LabelEncoder()\n",
    "for col in good_cols:\n",
    "    label_X_train[col] = labelEncoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col]  = labelEncoder.transform(X_valid[col])\n",
    "    \n",
    "model.fit(label_X_train,Y_train)\n",
    "predictions = model.predict(label_X_valid)\n",
    "mae2=mean_absolute_error(predictions,Y_valid)\n",
    "\n",
    "\n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "print(mae2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The MAE = 17575.29188</b>\n",
    "<br>\n",
    "<p>In the code cell above, for each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>11694</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>6600</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>13360</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>150</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>13704</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>468</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MSSubClass  MSZoning  LotArea  Street  LotShape  LandContour  LotConfig  \\\n",
       "Id                                                                             \n",
       "619          20         3    11694       1         3            3          4   \n",
       "871          20         3     6600       1         3            3          4   \n",
       "93           30         3    13360       1         0            1          4   \n",
       "818          20         3    13265       1         0            3          1   \n",
       "303          20         3    13704       1         0            3          0   \n",
       "\n",
       "     BldgType  HouseStyle  OverallQual  ...  WoodDeckSF  OpenPorchSF  \\\n",
       "Id                                      ...                            \n",
       "619         0           2            9  ...           0          108   \n",
       "871         0           2            5  ...           0            0   \n",
       "93          0           2            5  ...           0            0   \n",
       "818         0           2            8  ...         150           59   \n",
       "303         0           2            7  ...         468           81   \n",
       "\n",
       "     EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  \\\n",
       "Id                                                                              \n",
       "619              0          0          260         0        0       7    2007   \n",
       "871              0          0            0         0        0       8    2009   \n",
       "93              44          0            0         0        0       8    2009   \n",
       "818              0          0            0         0        0       7    2008   \n",
       "303              0          0            0         0        0       1    2006   \n",
       "\n",
       "     SaleCondition  \n",
       "Id                  \n",
       "619              5  \n",
       "871              4  \n",
       "93               4  \n",
       "818              4  \n",
       "303              4  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       int64\n",
       "MSZoning         int32\n",
       "LotArea          int64\n",
       "Street           int32\n",
       "LotShape         int32\n",
       "LandContour      int32\n",
       "LotConfig        int32\n",
       "BldgType         int32\n",
       "HouseStyle       int32\n",
       "OverallQual      int64\n",
       "OverallCond      int64\n",
       "YearBuilt        int64\n",
       "YearRemodAdd     int64\n",
       "ExterQual        int32\n",
       "BsmtFinSF1       int64\n",
       "BsmtFinSF2       int64\n",
       "BsmtUnfSF        int64\n",
       "TotalBsmtSF      int64\n",
       "CentralAir       int32\n",
       "1stFlrSF         int64\n",
       "2ndFlrSF         int64\n",
       "LowQualFinSF     int64\n",
       "GrLivArea        int64\n",
       "BsmtFullBath     int64\n",
       "BsmtHalfBath     int64\n",
       "FullBath         int64\n",
       "HalfBath         int64\n",
       "BedroomAbvGr     int64\n",
       "KitchenAbvGr     int64\n",
       "KitchenQual      int32\n",
       "TotRmsAbvGrd     int64\n",
       "Fireplaces       int64\n",
       "GarageCars       int64\n",
       "GarageArea       int64\n",
       "PavedDrive       int32\n",
       "WoodDeckSF       int64\n",
       "OpenPorchSF      int64\n",
       "EnclosedPorch    int64\n",
       "3SsnPorch        int64\n",
       "ScreenPorch      int64\n",
       "PoolArea         int64\n",
       "MiscVal          int64\n",
       "MoSold           int64\n",
       "YrSold           int64\n",
       "SaleCondition    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above, for each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables.\n",
    "\n",
    "### We refer to the number of unique entries of a categorical variable as the cardinality of that categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Street', 2),\n",
       " ('Utilities', 2),\n",
       " ('CentralAir', 2),\n",
       " ('LandSlope', 3),\n",
       " ('PavedDrive', 3),\n",
       " ('LotShape', 4),\n",
       " ('LandContour', 4),\n",
       " ('ExterQual', 4),\n",
       " ('KitchenQual', 4),\n",
       " ('MSZoning', 5),\n",
       " ('LotConfig', 5),\n",
       " ('BldgType', 5),\n",
       " ('ExterCond', 5),\n",
       " ('HeatingQC', 5),\n",
       " ('Condition2', 6),\n",
       " ('RoofStyle', 6),\n",
       " ('Foundation', 6),\n",
       " ('Heating', 6),\n",
       " ('Functional', 6),\n",
       " ('SaleCondition', 6),\n",
       " ('RoofMatl', 7),\n",
       " ('HouseStyle', 8),\n",
       " ('Condition1', 9),\n",
       " ('SaleType', 9),\n",
       " ('Exterior1st', 15),\n",
       " ('Exterior2nd', 16),\n",
       " ('Neighborhood', 25)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each value represents the no of unique values as well as the no of columns that will be required to uniquely one hot encode the values.\n",
    "\n",
    "For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset. For this reason, we typically will only one-hot encode columns with relatively low cardinality. Then, high cardinality columns can either be dropped from the dataset, or we can use label encoding.\n",
    "\n",
    "#### Entries added by one hot encoding = (Rows * unique values) -rows\n",
    "#### Entries added bu label encoding = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>METHOD 3 : <u>One-Hot encoding</u></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the OneHotEncoder class from scikit-learn to get one-hot encodings. There are a number of parameters that can be used to customize its behavior.\n",
    "\n",
    "* We set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data, and\n",
    "* setting sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n",
    "\n",
    "To use the encoder, we supply only the categorical columns that we want to be one-hot encoded.\n",
    "\n",
    "But, instead of encoding all of the categorical variables in the dataset,  create a one-hot encoding for columns with cardinality less than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be one-hot encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Exterior1st', 'Neighborhood', 'Exterior2nd']\n"
     ]
    }
   ],
   "source": [
    "# Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from one hot encoding: \n",
      "17525.345719178084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_train_highc_dropped = X_train.drop(high_cardinality_cols , axis = 1)\n",
    "X_valid_highc_dropped = X_valid.drop(high_cardinality_cols , axis = 1)\n",
    "\n",
    "#Apply OHE only to categorical data columns\n",
    "oh_encoder = OneHotEncoder(handle_unknown=\"ignore\",sparse = False)\n",
    "oh_cols_train = pd.DataFrame(oh_encoder.fit_transform(X_train_highc_dropped[low_cardinality_cols]))\n",
    "oh_cols_valid = pd.DataFrame(oh_encoder.transform(X_valid_highc_dropped[low_cardinality_cols]))\n",
    "\n",
    "#Putting back the index removed by one hot encoder\n",
    "oh_cols_train.index = X_train_highc_dropped.index\n",
    "oh_cols_valid.index = X_valid_highc_dropped.index\n",
    "\n",
    "#Remove categorical columns - as one hot enoded columns will  replace them\n",
    "num_X_train = X_train_highc_dropped.drop(low_cardinality_cols, axis=1)\n",
    "num_X_valid = X_valid_highc_dropped.drop(low_cardinality_cols, axis=1)\n",
    "\n",
    "# Add one hot encoding columns to numerical values\n",
    "oh_X_train = pd.concat([num_X_train,oh_cols_train], axis=1)\n",
    "oh_X_valid = pd.concat([num_X_valid,oh_cols_valid], axis=1)\n",
    "\n",
    "print(\"MAE from one hot encoding: \")\n",
    "model.fit(oh_X_train,Y_train)\n",
    "prediction = model.predict(oh_X_valid)\n",
    "mae_score=mean_absolute_error(prediction,Y_valid)\n",
    "print(mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE score by one hot encoding process = 17525.345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The world is filled with categorical data. You will be a much more effective data scientist if you know how to use this common data type!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIPELINES\n",
    "A critical skill for deploying (and even testing) complex model with preprocessing.\n",
    "Pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n",
    "\n",
    "Benefits of using pipelines:\n",
    "* Cleaner code\n",
    "* Fewer bugs\n",
    "* Easier to deploy\n",
    "* More options for model validation\n",
    "\n",
    "Steps to construct a pipeline:\n",
    "1. Define preprocessing steps\n",
    "2. define the model\n",
    "3. Create and evalutate the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed process to create a pipeline is given in this notebook:\n",
    "<a href=\"./Pipelines%20for%20machine%20learning.ipynb\">Pipelines for ML</a>\n",
    "This notebook is available in this directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION\n",
    "Using cross validation for better model performance\n",
    "\n",
    "For example, we could begin by dividing the data into 5 pieces, each 20% of the full dataset. In this case, we say that we have broken the data into 5 \"folds\".\n",
    "\n",
    "<img src=\"https://i.imgur.com/9k60cVA.png\"></img>\n",
    "<br>\n",
    "In one case we use first 20% as validation and the rest as trainning and in the next case we will use the second 20% as validation and the rest of the data as training. We will follow the same trend for subsequent steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>When to use cross validation</b>\n",
    "* Use when the dataset is small.\n",
    "* For larger datasets on using cross validation the time taken will be too much. FOr large dataset cross validation is not usually required.\n",
    "* Still then use cross validation to check if your model performs consistently over all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-51b18e18a24c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# mutilply by -1 to get +ve values since sklearn predicts -ve values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcvScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'function'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# mutilply by -1 to get +ve values since sklearn predicts -ve values\n",
    "cvScore = -1 * cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
